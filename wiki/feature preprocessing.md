# Feature preprocessing

## Data Discretization

### Blog: 
[Medium - Data Discretization](https://medium.com/codex/data-discretization-b5faa2b77f06)


## Feature engineering
https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114

### A Reference Guide to Feature Engineering Methods
https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods/notebook

### Resources:

#### Blogs:


#### Papers:
- (2017) [Learning Feature Engineering for Classification](https://www.ijcai.org/proceedings/2017/0352.pdf)

## Feature selection
### Google's guid
https://services.google.com/fh/files/misc/exploratory_data_analysis_for_feature_selection_in_machine_learning.pdf

- Low variance / entropy
- Highly correlated data

### Sklearning
https://scikit-learn.org/stable/modules/feature_selection.html

### Resources:

#### Blogs 
- [TWDS](https://towardsdatascience.com/beginners-guide-for-feature-selection-by-a-beginner-cd2158c5c36a)
- [Motivations for feature preprocessing](https://neptune.ai/blog/feature-selection-methods)
- [ML Mastery](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)
  - I would personally stick to pearson/spearmans for numerical and chi squared for categorical

#### Papers:
- (2018) [On the Stability of Feature Selection Algorithms](https://www.jmlr.org/papers/volume18/17-514/17-514.pdf)
  


 


 